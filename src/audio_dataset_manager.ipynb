{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import re\n",
    "import subprocess\n",
    "from pydub import AudioSegment\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AudioProcess_Config():\n",
    "    \"\"\"Class to store the necessary variables to processing the audio\"\"\"\n",
    "    filepath: str\n",
    "    output_folder: str\n",
    "    usable_folder: str\n",
    "    not_usable_folder: str\n",
    "    time_threshold: float\n",
    "    whisper_model: str\n",
    "    prefix : str\n",
    "    \n",
    "class AudioProcessor():\n",
    "    def detect_silences(self, config, decibel=\"-23dB\"):\n",
    "        \n",
    "        # Executing ffmpeg to detect silences\n",
    "        command = [\"ffmpeg\",\"-i\",config.filepath,\"-af\",f\"silencedetect=n={decibel}:d={str(config.time_threshold)}\",\"-f\",\"null\",\"-\"]\n",
    "        out = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "        stdout, stderr = out.communicate()\n",
    "\n",
    "        # Decoding and splitting ffmpeg output\n",
    "        output = stdout.decode(\"utf-8\")\n",
    "        silence_info = output.split('[silencedetect @')\n",
    "        silence_starts = []\n",
    "        silence_ends = []\n",
    "\n",
    "        if len(silence_info) > 1:\n",
    "        \n",
    "            # Process each detected silence fragment \n",
    "            for index, segment in enumerate(silence_info[1:], start=1):\n",
    "                segment_details = segment.split(']')[1]\n",
    "                time_values = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", segment_details)\n",
    "        \n",
    "                if time_values:\n",
    "                    time = float(time_values[0])\n",
    "        \n",
    "                    # Checking whether the time should be either the start or end time according to where we are in the iteration\n",
    "                    if index % 2 == 0 :\n",
    "                        silence_ends.append(time)\n",
    "                    else:\n",
    "                        silence_starts.append(time)\n",
    "        \n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        else :\n",
    "            return('No silence was detected')\n",
    "\n",
    "        return list(zip(silence_starts, silence_ends))\n",
    "\n",
    "def define_process_config(filepath, time_threshold, whisper_model, output_folder, prefix):\n",
    "    usable_folder = os.path.join(output_folder, 'Usable_Audios')\n",
    "    not_usable_folder = os.path.join(output_folder, 'Not_Usable_Audios')\n",
    "\n",
    "    return AudioProcess_Config(\n",
    "        filepath=filepath,\n",
    "        output_folder=output_folder,\n",
    "        usable_folder=usable_folder,\n",
    "        not_usable_folder=not_usable_folder,\n",
    "        time_threshold=time_threshold,\n",
    "        whisper_model=whisper_model,\n",
    "        prefix=prefix\n",
    "        \n",
    "    )\n",
    "    \n",
    "def main(filepath, time_threshold, whisper_model, output_folder, prefix=None):\n",
    "    process_config = define_process_config(filepath, time_threshold, whisper_model, output_folder, prefix)\n",
    "    ap = AudioProcessor()\n",
    "\n",
    "    silence_list = ap.detect_silences(process_config)\n",
    "\n",
    "    return silence_list\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "demo = gr.Interface(\n",
    "    fn=main,\n",
    "    inputs=[\n",
    "        gr.Audio(sources=\"upload\", \n",
    "                 type=\"filepath\"),\n",
    "        gr.Number(label = 'Time Threshold',\n",
    "                 info = 'Choose the approximate duration of a silence in the audio'), \n",
    "        gr.Dropdown(\n",
    "            [\n",
    "                \"Tiny\",\n",
    "                \"Base\",\n",
    "                \"Medium\",\n",
    "                \"Large\"\n",
    "            ],\n",
    "            label = \"Whisper model\",\n",
    "            info = \"Choose the Whisper model with which you want to do the retranscription\"\n",
    "        ),\n",
    "     \n",
    "          gr.Textbox(\n",
    "            label = 'Output Folder',\n",
    "            info = 'Type the path where you want to output the segmented audios)'\n",
    "        ),\n",
    "           gr.Textbox(\n",
    "            label = 'Prefix',\n",
    "            info = 'Choose a prefix for your extracted audio segments (like the name and chapter of the book)'\n",
    "        )\n",
    "        \n",
    "        \n",
    "    \n",
    "    ],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
